{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overcoming a Theoretical Limitation of Self-Attention "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replication of experiments on FIRST language learning from [Overcoming a Theoretical Limitation of Self-Attention  (Chiang and Cholak, 2022)](https://arxiv.org/pdf/2202.12172.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.transformer import FirstTransformer, FirstExactTransformer\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the following to true to use the FirstExactTransformer, and to false to use FirstTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXACT = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning FIRST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training parameters as in the original paper. Citing from (David Chiang and Peter Cholak, 2020):\n",
    "> We used `d_model` = 16 for word encodings, self-attention, and FFNN outputs, and `d_FFNN` = 64 for FFNN hidden layers. We used layer normalization (ε = 10^−5) after residual connections. We used PyTorch’s default initialization and trained using Adam (Kingma and Ba, 2015) with learning rate 3 × 10^−4 (Karpathy, 2016). We did not use dropout, as it did not seem to help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"0\", \"1\", \"$\"]\n",
    "\n",
    "epochs = 20\n",
    "layers = 2\n",
    "heads = 1 \n",
    "if EXACT:\n",
    "    d_model = 6 # do not change this!\n",
    "else:\n",
    "    d_model = 16\n",
    "d_ffnn = 64  \n",
    "eps = 1e-5 # value added to denominator in layer normalization\n",
    "scaled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Transformer to learn FIRST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = \\\n",
    "    FirstExactTransformer(len(vocab), d_model) if EXACT \\\n",
    "    else FirstTransformer(len(vocab), layers, heads, d_model, d_ffnn, scaled, eps)\n",
    "optim = torch.optim.Adam(transformer.parameters(), lr=0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6625"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in transformer.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model trainer and train the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train acc: 0.48 Train loss: 70.82251560688019, Test acc: 0.48 Test loss: 70.53018110990524\n",
      "[Epoch 2] Train acc: 0.62 Train loss: 68.22715350985527, Test acc: 0.48 Test loss: 70.05497199296951\n",
      "[Epoch 3] Train acc: 0.68 Train loss: 62.594036757946014, Test acc: 0.45 Test loss: 70.20428496599197\n",
      "[Epoch 4] Train acc: 0.73 Train loss: 58.06704166531563, Test acc: 0.47 Test loss: 69.63091653585434\n",
      "[Epoch 5] Train acc: 0.85 Train loss: 43.17119553685188, Test acc: 0.42 Test loss: 84.34749820828438\n",
      "[Epoch 6] Train acc: 0.99 Train loss: 14.547088660299778, Test acc: 0.51 Test loss: 121.22422835230827\n",
      "[Epoch 7] Train acc: 1.0 Train loss: 7.5821383446455, Test acc: 0.54 Test loss: 121.47382888942957\n",
      "[Epoch 8] Train acc: 1.0 Train loss: 6.122796632349491, Test acc: 0.58 Test loss: 119.17250917106867\n",
      "[Epoch 9] Train acc: 1.0 Train loss: 5.190062433481216, Test acc: 0.51 Test loss: 146.14726735278964\n",
      "[Epoch 10] Train acc: 1.0 Train loss: 4.49142375215888, Test acc: 0.48 Test loss: 162.1278810314834\n",
      "[Epoch 11] Train acc: 1.0 Train loss: 3.8924187012016773, Test acc: 0.52 Test loss: 157.50577096641064\n",
      "[Epoch 12] Train acc: 1.0 Train loss: 3.302965333685279, Test acc: 0.38 Test loss: 211.26003355905414\n",
      "[Epoch 13] Train acc: 1.0 Train loss: 2.849249344319105, Test acc: 0.58 Test loss: 149.91707264445722\n",
      "[Epoch 14] Train acc: 1.0 Train loss: 2.5163382906466722, Test acc: 0.49 Test loss: 188.628914443776\n",
      "[Epoch 15] Train acc: 1.0 Train loss: 2.1887698471546173, Test acc: 0.58 Test loss: 161.5023883804679\n",
      "[Epoch 16] Train acc: 1.0 Train loss: 1.915126321837306, Test acc: 0.54 Test loss: 182.80715756304562\n",
      "[Epoch 17] Train acc: 1.0 Train loss: 1.6632999181747437, Test acc: 0.52 Test loss: 196.70620844326913\n",
      "[Epoch 18] Train acc: 1.0 Train loss: 1.4768546232953668, Test acc: 0.56 Test loss: 186.06943056732416\n",
      "[Epoch 19] Train acc: 1.0 Train loss: 1.2986966017633677, Test acc: 0.49 Test loss: 221.53713019751012\n",
      "[Epoch 20] Train acc: 1.0 Train loss: 1.1487172497436404, Test acc: 0.54 Test loss: 205.27205955516547\n"
     ]
    }
   ],
   "source": [
    "from src.trainer import Trainer\n",
    "from src.dataset import Dataset\n",
    "\n",
    "trainset = Dataset(0, 100, 10, random_seed=42, train=True, data_type='first', variable_lenght=False)\n",
    "testset = Dataset(0, 100, 1000,  random_seed=42,  train=False, data_type='first', variable_lenght=False)\n",
    "\n",
    "trainer = Trainer(0, transformer, optim, vocab, epochs, trainset, testset, verbose=1)\n",
    "train_l, val_l, train_acc, val_acc = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(range(epochs+1), train_l, color='blue', lw=2)\n",
    "plt.plot(range(epochs+1), val_l, color='orange', lw=2)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(range(epochs+1), train_acc, color='blue', lw=2)\n",
    "plt.plot(range(epochs+1), val_acc, color='orange', lw=2)\n",
    "plt.ylim([0, 1.1])\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, frameon=False, loc='lower center',  ncol=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('aflt-proj')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e79681ef656beecc23f40fa8189b5ee6b5f2b38db808810ba34977390d131671"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
