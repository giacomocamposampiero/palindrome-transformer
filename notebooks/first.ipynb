{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overcoming a Theoretical Limitation of Self-Attention "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replication of experiments on FIRST language learning from [Overcoming a Theoretical Limitation of Self-Attention  (Chiang and Cholak, 2022)](https://arxiv.org/pdf/2202.12172.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../')\n",
    "from src.transformer import FirstTransformer, FirstExactTransformer\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the following to true to use the FirstExactTransformer, and to false to use FirstTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXACT = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning FIRST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training parameters as in the original paper. Citing from (David Chiang and Peter Cholak, 2020):\n",
    "> We used `d_model` = 16 for word encodings, self-attention, and FFNN outputs, and `d_FFNN` = 64 for FFNN hidden layers. We used layer normalization (ε = 10^−5) after residual connections. We used PyTorch’s default initialization and trained using Adam (Kingma and Ba, 2015) with learning rate 3 × 10^−4 (Karpathy, 2016). We did not use dropout, as it did not seem to help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"0\", \"1\", \"$\"]\n",
    "\n",
    "epochs = 20\n",
    "layers = 2\n",
    "heads = 1 \n",
    "if EXACT:\n",
    "    d_model = 6 # do not change this!\n",
    "else:\n",
    "    d_model = 16\n",
    "d_ffnn = 64  \n",
    "eps = 1e-5 # value added to denominator in layer normalization\n",
    "scaled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Transformer to learn FIRST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = \\\n",
    "    FirstExactTransformer(len(vocab), d_model) if EXACT \\\n",
    "    else FirstTransformer(len(vocab), layers, heads, d_model, d_ffnn, scaled, eps)\n",
    "optim = torch.optim.Adam(transformer.parameters(), lr=0.0003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model trainer and train the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.trainer import Trainer\n",
    "from src.dataset import Dataset\n",
    "\n",
    "trainset = Dataset(0, 100, 10, random_seed=42, train=True, data_type='first', variable_lenght=False)\n",
    "testset = Dataset(0, 100, 1000,  random_seed=42,  train=False, data_type='first', variable_lenght=False)\n",
    "\n",
    "trainer = Trainer(0, transformer, optim, vocab, epochs, trainset, testset, verbose=1)\n",
    "train_l, val_l, train_acc, val_acc = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(range(epochs), train_l, color='blue', lw=2)\n",
    "plt.plot(range(epochs), val_l, color='orange', lw=2)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(range(epochs), train_acc, color='blue', lw=2)\n",
    "plt.plot(range(epochs), val_acc, color='orange', lw=2)\n",
    "plt.ylim([0, 1.1])\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, frameon=False, loc='lower center',  ncol=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('aflt-proj')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e79681ef656beecc23f40fa8189b5ee6b5f2b38db808810ba34977390d131671"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
