{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overcoming a Theoretical Limitation of Soft-Attention \n",
    "Example notebook on learning experiments for One."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../')\n",
    "from src.transformer import StandardTransformer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning One"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training parameters.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"0\", \"1\", \"$\"]\n",
    "\n",
    "epochs = 30\n",
    "layers = 2\n",
    "heads = 1 \n",
    "d_model = 16\n",
    "d_ffnn = 64  \n",
    "eps = 1e-5 # value added to denominator in layer normalization\n",
    "scaled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = StandardTransformer(len(vocab), layers, heads, d_model, d_ffnn, scaled, eps, positional=\"first\")\n",
    "optim = torch.optim.Adam(transformer.parameters(), lr=0.0003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model trainer and train the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.trainer import Trainer\n",
    "from src.dataset import Dataset\n",
    "\n",
    "trainset = Dataset(0, 100, 100, random_seed=42, train=True, data_type='one', variable_lenght=True)\n",
    "testset = Dataset(0, 100, 500,  random_seed=42,  train=False, data_type='one', variable_lenght=False)\n",
    "\n",
    "trainer = Trainer(0, transformer, optim, vocab, epochs, trainset, testset, verbose=0)\n",
    "train_l, val_l, train_acc, val_acc = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(range(epochs), train_l, color='blue', lw=2, label=\"Train loss\")\n",
    "plt.plot(range(epochs), val_l, color='orange', lw=2, label=\"Validation loss\")\n",
    "plt.yscale('log')\n",
    "plt.ylabel(\"Loss per digit\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(range(epochs), train_acc, color='blue', lw=2, label=\"Train accuracy\")\n",
    "plt.plot(range(epochs), val_acc, color='orange', lw=2, label=\"Validation accuracy\")\n",
    "plt.ylim([0, 1.1])\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('aflt-proj')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e79681ef656beecc23f40fa8189b5ee6b5f2b38db808810ba34977390d131671"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
