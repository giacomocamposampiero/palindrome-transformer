{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overcoming a Theoretical Limitation of Self-Attention "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replication of experiments on FIRST language learning from [Overcoming a Theoretical Limitation of Self-Attention  (Chiang and Cholak, 2022)](https://arxiv.org/pdf/2202.12172.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../')\n",
    "from src.transformer import OneTransformer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning FIRST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training parameters as in the original paper. Citing from (David Chiang and Peter Cholak, 2020):\n",
    "> We used `d_model` = 16 for word encodings, self-attention, and FFNN outputs, and `d_FFNN` = 64 for FFNN hidden layers. We used layer normalization (ε = 10^−5) after residual connections. We used PyTorch’s default initialization and trained using Adam (Kingma and Ba, 2015) with learning rate 3 × 10^−4 (Karpathy, 2016). We did not use dropout, as it did not seem to help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"0\", \"1\", \"$\"]\n",
    "\n",
    "epochs = 50\n",
    "layers = 2\n",
    "heads = 1 \n",
    "d_model = 16\n",
    "d_ffnn = 64  \n",
    "eps = 1e-5 # value added to denominator in layer normalization\n",
    "scaled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Transformer to learn FIRST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = OneTransformer(len(vocab), layers, heads, d_model, d_ffnn, scaled, eps)\n",
    "optim = torch.optim.Adam(transformer.parameters(), lr=0.0003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model trainer and train the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train acc: 0.65 Train loss: 62.76117920875549, Test acc: 0.74 Test loss: 58.77211570739746\n",
      "[Epoch 2] Train acc: 0.66 Train loss: 64.88557317852974, Test acc: 0.66 Test loss: 65.44172233343124\n",
      "[Epoch 3] Train acc: 0.73 Train loss: 58.81425270438194, Test acc: 0.67 Test loss: 63.940348505973816\n",
      "[Epoch 4] Train acc: 0.62 Train loss: 68.54289397597313, Test acc: 0.78 Test loss: 61.27927923202515\n",
      "[Epoch 5] Train acc: 0.68 Train loss: 63.727036476135254, Test acc: 0.61 Test loss: 67.2500813305378\n",
      "[Epoch 6] Train acc: 0.66 Train loss: 64.97652953863144, Test acc: 0.64 Test loss: 65.43881437182426\n",
      "[Epoch 7] Train acc: 0.66 Train loss: 64.63837903738022, Test acc: 0.67 Test loss: 63.75513228774071\n",
      "[Epoch 8] Train acc: 0.63 Train loss: 66.46105140447617, Test acc: 0.58 Test loss: 68.47598570585251\n",
      "[Epoch 9] Train acc: 0.72 Train loss: 59.58387789130211, Test acc: 0.68 Test loss: 62.95991292595863\n",
      "[Epoch 10] Train acc: 0.74 Train loss: 58.020698100328445, Test acc: 0.61 Test loss: 74.79177495837212\n",
      "[Epoch 11] Train acc: 0.74 Train loss: 57.58317090570927, Test acc: 0.6 Test loss: 75.99259102344513\n",
      "[Epoch 12] Train acc: 0.6 Train loss: 70.29056295752525, Test acc: 0.72 Test loss: 60.55878794193268\n",
      "[Epoch 13] Train acc: 0.64 Train loss: 65.57778489589691, Test acc: 0.67 Test loss: 63.38875192403793\n",
      "[Epoch 14] Train acc: 0.69 Train loss: 61.95789137482643, Test acc: 0.63 Test loss: 71.87360498309135\n",
      "[Epoch 15] Train acc: 0.67 Train loss: 62.56310427188873, Test acc: 0.62 Test loss: 75.72037769854069\n",
      "[Epoch 16] Train acc: 0.7 Train loss: 60.21597856283188, Test acc: 0.61 Test loss: 80.2761324942112\n",
      "[Epoch 17] Train acc: 0.66 Train loss: 59.898622304201126, Test acc: 0.67 Test loss: 78.39865164458752\n",
      "[Epoch 18] Train acc: 0.68 Train loss: 56.36023862659931, Test acc: 0.47 Test loss: 77.98922400176525\n",
      "[Epoch 19] Train acc: 0.89 Train loss: 41.910259164869785, Test acc: 0.65 Test loss: 94.27315302938223\n",
      "[Epoch 20] Train acc: 1.0 Train loss: 16.159661754965782, Test acc: 0.63 Test loss: 102.05633097141981\n",
      "[Epoch 21] Train acc: 1.0 Train loss: 7.2777298130095005, Test acc: 0.59 Test loss: 119.91273286938667\n",
      "[Epoch 22] Train acc: 1.0 Train loss: 5.25900461897254, Test acc: 0.75 Test loss: 80.04653650149703\n",
      "[Epoch 23] Train acc: 1.0 Train loss: 4.2462593503296375, Test acc: 0.65 Test loss: 115.98871375992894\n",
      "[Epoch 24] Train acc: 1.0 Train loss: 3.529577447101474, Test acc: 0.7 Test loss: 104.82443603873253\n",
      "[Epoch 25] Train acc: 1.0 Train loss: 2.938459038734436, Test acc: 0.67 Test loss: 120.25773169845343\n",
      "[Epoch 26] Train acc: 1.0 Train loss: 2.4786156490445137, Test acc: 0.6 Test loss: 151.517768278718\n",
      "[Epoch 27] Train acc: 1.0 Train loss: 2.1109379306435585, Test acc: 0.62 Test loss: 149.40422647632658\n",
      "[Epoch 28] Train acc: 1.0 Train loss: 1.7890850054100156, Test acc: 0.58 Test loss: 171.17948255315423\n",
      "[Epoch 29] Train acc: 1.0 Train loss: 1.547847406938672, Test acc: 0.72 Test loss: 118.61131601314992\n",
      "[Epoch 30] Train acc: 1.0 Train loss: 1.3338060127571225, Test acc: 0.75 Test loss: 109.3189204717055\n",
      "[Epoch 31] Train acc: 1.0 Train loss: 1.1686182469129562, Test acc: 0.67 Test loss: 147.9104380654171\n",
      "[Epoch 32] Train acc: 1.0 Train loss: 1.0298206768929958, Test acc: 0.6 Test loss: 184.18410328309983\n",
      "[Epoch 33] Train acc: 1.0 Train loss: 0.9067846722900867, Test acc: 0.58 Test loss: 198.51477835793048\n",
      "[Epoch 34] Train acc: 1.0 Train loss: 0.8087214529514313, Test acc: 0.74 Test loss: 126.3432222334668\n",
      "[Epoch 35] Train acc: 1.0 Train loss: 0.7178019168786705, Test acc: 0.63 Test loss: 183.56189483590424\n",
      "[Epoch 36] Train acc: 1.0 Train loss: 0.6418437762185931, Test acc: 0.68 Test loss: 162.5481944810599\n",
      "[Epoch 37] Train acc: 1.0 Train loss: 0.5728726335801184, Test acc: 0.72 Test loss: 145.25273274676874\n",
      "[Epoch 38] Train acc: 1.0 Train loss: 0.5142106954008341, Test acc: 0.68 Test loss: 169.60559921804816\n",
      "[Epoch 39] Train acc: 1.0 Train loss: 0.4631407540291548, Test acc: 0.7 Test loss: 162.1551308389753\n",
      "[Epoch 40] Train acc: 1.0 Train loss: 0.41727052303031087, Test acc: 0.66 Test loss: 187.19602505862713\n",
      "[Epoch 41] Train acc: 1.0 Train loss: 0.3792366897687316, Test acc: 0.76 Test loss: 134.65295877680182\n",
      "[Epoch 42] Train acc: 1.0 Train loss: 0.34260995453223586, Test acc: 0.71 Test loss: 165.56476745312102\n",
      "[Epoch 43] Train acc: 1.0 Train loss: 0.311099442653358, Test acc: 0.6 Test loss: 231.8484985041432\n",
      "[Epoch 44] Train acc: 1.0 Train loss: 0.2826196027453989, Test acc: 0.73 Test loss: 159.19410569034517\n",
      "[Epoch 45] Train acc: 1.0 Train loss: 0.2579195462167263, Test acc: 0.65 Test loss: 209.58037097542547\n",
      "[Epoch 46] Train acc: 1.0 Train loss: 0.235574999358505, Test acc: 0.68 Test loss: 194.37657947512344\n",
      "[Epoch 47] Train acc: 1.0 Train loss: 0.2155137222725898, Test acc: 0.66 Test loss: 209.4242258053273\n",
      "[Epoch 48] Train acc: 1.0 Train loss: 0.19848388980608433, Test acc: 0.68 Test loss: 200.02924757718574\n",
      "[Epoch 49] Train acc: 1.0 Train loss: 0.18121017375960946, Test acc: 0.62 Test loss: 240.8616117866477\n",
      "[Epoch 50] Train acc: 1.0 Train loss: 0.16722716751974076, Test acc: 0.64 Test loss: 231.11224832292646\n"
     ]
    }
   ],
   "source": [
    "from src.trainer import Trainer\n",
    "from src.dataset import Dataset\n",
    "\n",
    "trainset = Dataset(0, 100, 100, random_seed=42, train=True, data_type='one', variable_lenght=False)\n",
    "testset = Dataset(0, 100, 10,  random_seed=42,  train=False, data_type='one', variable_lenght=False)\n",
    "\n",
    "trainer = Trainer(0, transformer, optim, vocab, epochs, trainset, testset, verbose=1)\n",
    "train_l, val_l, train_acc, val_acc = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(range(epochs), train_l, color='blue', lw=2, label=\"Train loss\")\n",
    "plt.plot(range(epochs), val_l, color='orange', lw=2, label=\"Validation loss\")\n",
    "plt.yscale('log')\n",
    "ax = plt.gca()\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, frameon=False, loc='lower center',  ncol=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(range(epochs), train_acc, color='blue', lw=2, label=\"Train accuracy\")\n",
    "plt.plot(range(epochs), val_acc, color='orange', lw=2, label=\"Validation accuracy\")\n",
    "plt.ylim([0, 1.1])\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, frameon=False, loc='lower center',  ncol=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('aflt-proj')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e79681ef656beecc23f40fa8189b5ee6b5f2b38db808810ba34977390d131671"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
